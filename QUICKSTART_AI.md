# ğŸš€ Quick Start - AI Slide Generation

HÆ°á»›ng dáº«n nhanh Ä‘á»ƒ cháº¡y tÃ­nh nÄƒng táº¡o slides báº±ng AI local.

## âœ… Checklist

- [ ] Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t
- [ ] Ollama service Ä‘ang cháº¡y
- [ ] Model Ä‘Ã£ Ä‘Æ°á»£c pull vá»
- [ ] Backend dependencies Ä‘Ã£ cÃ i
- [ ] Backend Ä‘ang cháº¡y

## ğŸ“‹ CÃ¡c bÆ°á»›c thá»±c hiá»‡n

### 1. CÃ i Ä‘áº·t Ollama

**macOS:**
```bash
brew install ollama
```

**Kiá»ƒm tra version:**
```bash
ollama --version
```

### 2. Khá»Ÿi Ä‘á»™ng Ollama service

```bash
ollama serve
```

Giá»¯ terminal nÃ y cháº¡y, má»Ÿ terminal má»›i cho cÃ¡c bÆ°á»›c sau.

### 3. Pull model vá» mÃ¡y

```bash
# Model khuyáº¿n nghá»‹ (3.8GB)
ollama pull llama3.2

# Hoáº·c model nhá» hÆ¡n náº¿u mÃ¡y yáº¿u (1.5GB)
ollama pull llama3.2:1b
```

Äá»£i model download xong (cÃ³ thá»ƒ máº¥t 5-10 phÃºt).

**Kiá»ƒm tra:**
```bash
ollama list
```

Pháº£i tháº¥y model `llama3.2` trong danh sÃ¡ch.

### 4. CÃ i Ä‘áº·t Backend dependencies

```bash
cd backend
npm install
```

### 5. Cháº¡y Backend

```bash
npm run dev
```

Backend sáº½ cháº¡y á»Ÿ `http://localhost:3001`

### 6. Test thá»­

**Option A: Test báº±ng script**
```bash
cd backend
node src/scripts/test_ollama.js
```

**Option B: Test báº±ng curl**
```bash
# Health check
curl http://localhost:3001/ai/health

# Generate slides (cáº§n token)
curl -X POST http://localhost:3001/ai/generate-slides \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "topic": "AI trong giÃ¡o dá»¥c",
    "slideCount": 3,
    "tone": "professional",
    "language": "vi"
  }'
```

**Option C: Test báº±ng HTML form**
1. Má»Ÿ file `test-ai-slides.html` trong trÃ¬nh duyá»‡t
2. Login trÆ°á»›c Ä‘á»ƒ cÃ³ JWT token
3. Nháº­p thÃ´ng tin vÃ  click "Generate Slides"

## ğŸ”§ Troubleshooting

### "Cannot connect to Ollama"
```bash
# Kiá»ƒm tra Ollama cÃ³ cháº¡y khÃ´ng
curl http://localhost:11434

# Náº¿u khÃ´ng, start láº¡i
ollama serve
```

### "Model not found"
```bash
# Kiá»ƒm tra model Ä‘Ã£ cÃ³ chÆ°a
ollama list

# Pull láº¡i náº¿u chÆ°a cÃ³
ollama pull llama3.2
```

### "AI generation timeout"
- DÃ¹ng model nhá» hÆ¡n: `llama3.2:1b`
- Giáº£m sá»‘ slide xuá»‘ng (2-5 slides)
- Äá»£i thÃªm thá»i gian (láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t 30-60s)

### "Port 3001 already in use"
```bash
# TÃ¬m process Ä‘ang dÃ¹ng port
lsof -ti:3001

# Kill process Ä‘Ã³
kill -9 $(lsof -ti:3001)

# Hoáº·c Ä‘á»•i port trong .env
PORT=3002
```

## ğŸ“Š Performance

| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| llama3.2:1b | 1.5GB | âš¡âš¡âš¡ Fast | â­â­ OK |
| llama3.2 | 3.8GB | âš¡âš¡ Medium | â­â­â­ Good |
| mistral | 4.7GB | âš¡ Slow | â­â­â­â­ Great |

**Khuyáº¿n nghá»‹:** DÃ¹ng `llama3.2` cho cÃ¢n báº±ng tá»‘c Ä‘á»™ + cháº¥t lÆ°á»£ng.

## ğŸ¯ Endpoints

### POST `/ai/generate-slides`
Táº¡o slides vÃ  export PPTX

**Request:**
```json
{
  "topic": "Chá»§ Ä‘á»",
  "slideCount": 5,
  "tone": "professional",
  "language": "vi",
  "exportFormat": "pptx"
}
```

**Response:**
```json
{
  "success": true,
  "slides": [...],
  "file": {
    "filename": "presentation_xxx.pptx",
    "downloadUrl": "/uploads/presentations/...",
    "filepath": "..."
  }
}
```

### GET `/ai/health`
Kiá»ƒm tra Ollama status

**Response:**
```json
{
  "status": "healthy",
  "models": ["llama3.2:latest"]
}
```

## ğŸ“š TÃ i liá»‡u Ä‘áº§y Ä‘á»§

Xem: [AI_INTEGRATION_GUIDE.md](./AI_INTEGRATION_GUIDE.md)

## ğŸ’¬ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra logs á»Ÿ terminal
2. Xem [Troubleshooting](#troubleshooting)
3. Äá»c [AI_INTEGRATION_GUIDE.md](./AI_INTEGRATION_GUIDE.md)

---

**Happy Coding! ğŸ‰**
